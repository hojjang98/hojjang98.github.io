---
layout: post
title: "Paper Review: MobileNetV2 DAY 3"
date: 2025-07-03
categories: paper_review
mathjax: true
---

## üìå Paper Info

- **Title**: *MobileNetV2: Inverted Residuals and Linear Bottlenecks*  
- **Authors**: Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen  
- **Link**: [arXiv 1801.04381](https://arxiv.org/abs/1801.04381)  
- **Published**: 2018 (Google Research)  
- **Code**: Available in TensorFlow-Slim  

---

## üß† Day 3 Review ‚Äì Experiments, Applications, and Conclusions

### ‚úÖ Step 1: Architecture Expansion

The final MobileNetV2 network consists of:

```bash
Initial 3√ó3 Conv
‚Üí Repeating Bottleneck Blocks (19 times)
‚Üí Final 1√ó1 Conv + Global Average Pooling + FC layer
```

- Each block uses an **expansion factor t = 6**  
- All convolutions are 3√ó3 or 1√ó1  
- BatchNorm & Dropout applied where appropriate  
- ReLU6 for activation (except final linear projection)

---

### ‚úÖ Step 2: Experimental Results Summary

#### üìç ImageNet Classification

| Model           | Top-1 Acc | MACs   | Params |
|----------------|-----------|--------|--------|
| MobileNetV1     | 70.6%     | 575M   | 4.2M   |
| **MobileNetV2** | **71.8%** | 300M   | 3.4M   |

‚Üí V2 achieves **better accuracy** with nearly **half the computation**.

#### üìç Object Detection (COCO, with SSDLite)

| Model                | mAP   | Latency |
|---------------------|-------|---------|
| MobileNetV1 + SSD   | 19.3  | 27 ms   |
| **MobileNetV2 + SSDLite** | **22.1** | **19 ms** |

‚Üí V2 provides **higher mAP** and **faster inference**.

#### üìç Memory Efficiency

In Table 2 (Fig. 2 in the paper), MobileNetV2 shows **peak memory usage < 400K** during inference.  
This is **lower than ResNet-50, VGG, Inception**, and other baselines.

---

### ‚úÖ Step 3: Applications

- **Object Detection**: Used in real-time detectors like **SSDLite**  
- **Semantic Segmentation**: Combined with DeepLabv3 for mobile segmentation  
- **Mobile Transfer Learning**: Widely used for fine-tuning on edge devices

---

## ‚úÖ Key Insights (3-Line Summary)

- MobileNetV2 achieves strong accuracy with low memory and compute requirements.  
- The architecture outperforms V1 and competes with heavier models in speed and accuracy.  
- It is ideal for on-device inference tasks such as detection and segmentation.

---

## üìò New Terms

- **MACs (Multiply-Accumulate Ops)**: A proxy for computation cost  
- **mAP (mean Average Precision)**: Detection accuracy averaged across IoU thresholds  
- **Materialized Memory**: Memory required to hold intermediate activations during inference

---

## üóÇ GitHub Repository

Visual summary + experimental table:  
üîó [github.com/hojjang98/Paper-Review](https://github.com/hojjang98/Paper-Review/blob/main/vision/01_mobilenetv2/summary.md)

---

## üí≠ Reflections

The experiments confirm that MobileNetV2 is not just lightweight in theory, but in practice.  
Its memory efficiency and speed make it one of the most impactful mobile architectures of its time.  
I'm especially impressed with how well it balances performance and hardware constraints.

---

