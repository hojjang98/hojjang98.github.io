---
layout: post  
title: "Paper Review: DoWhy – DAY 2"  
date: 2025-08-05  
categories: paper_review  
mathjax: true  
---

> 📚 [https://arxiv.org/abs/2011.04216](https://arxiv.org/abs/2011.04216)  
> 🧑‍💻 GitHub: [microsoft/dowhy](https://github.com/microsoft/dowhy)

## ✅ Day 2 – From Theory to Practice: DoWhy in Action

Today’s reading focused on how the DoWhy framework is implemented in code and applied to real-world problems.  
This part bridges the philosophical motivations of causal inference with hands-on analysis,  
showing how assumptions, estimations, and robustness checks come together in one cohesive pipeline.

---

## 🧠 What I Learned – Turning Causal Ideas into Executable Pipelines

### 🔧 Implementation Structure

DoWhy formalizes causal inference into four programmable steps, centered around the `CausalModel` class:

| Step | Function | Description |
|------|----------|-------------|
| **Model** | `CausalModel()` | Define treatment, outcome, and confounders with a causal graph |
| **Identify** | `identify_effect()` | Use criteria (e.g., backdoor) to check identifiability |
| **Estimate** | `estimate_effect()` | Compute causal effect using regression, matching, or ML estimators |
| **Refute** | `refute_estimate()` | Validate results via robustness tests (e.g., placebo, subset) |

> 🛠️ The modular design forces analysts to **declare, test, and challenge** their assumptions — not just compute effects.

---

### 📊 API Walkthrough (No Code Yet)

A typical DoWhy analysis flows as:

1. Load your dataset and define treatment/outcome/confounders  
2. Build a `CausalModel` and visualize the graph  
3. Run `identify_effect()` to confirm the estimand  
4. Use `estimate_effect()` with a chosen method (e.g., linear regression, propensity score matching)  
5. Run `refute_estimate()` to see if the result is stable under small perturbations

Each stage can be flexibly adapted to your data, assumptions, and methods — without leaving the structured causal reasoning flow.

---

### 🧪 Case Studies from the Paper

The authors demonstrate DoWhy across domains:

| Domain | Treatment → Outcome |
|--------|---------------------|
| Medical | Smoking → Lung Cancer |
| Education | Online Course → Exam Score |
| Policy | Program Participation → Employment |

In each example:
- The **causal graph** defines assumptions  
- **Backdoor paths** are adjusted for identifiability  
- **Statistical estimators** compute the causal effect  
- **Refutation tests** reveal potential weaknesses

> 🔍 These case studies show that DoWhy is not just a toy — it's a **practical toolkit** for complex social and health systems.

---

### 🧠 Key Insights

- DoWhy transforms causal inference into a **transparent, testable, and repeatable** pipeline  
- It works **with**, not against, existing ML/Stats tools (e.g., OLS, Random Forest, Instrumental Variables)  
- The framework promotes **healthy skepticism** by making refutation tests a first-class citizen  
- Analysts must **own their assumptions** — no more hand-waving causal claims

---

## 📝 Takeaways

- DoWhy’s implementation reinforces that causal reasoning is a **discipline**, not just a toolkit  
- Its API encourages the habit of **declaring your model** and **testing your belief** in it  
- Whether for healthcare, education, or economics, DoWhy enables analysts to move from "what happens" to "why it happens"  
- Tomorrow, I’ll test this pipeline on a real dataset and evaluate the end-to-end flow myself!

---
